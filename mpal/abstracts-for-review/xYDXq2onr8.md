# For review only---DO NOT CITE

## Abstract 14

### ‘Long nose’ and ‘naso lungo’: establishing the need for retrodiction in computational models of word learning

Distributional information plays a crucial role for many tasks in language acquisition (Saffran, 2020). The distributional regularities in a given language are necessarily subject to typological variability, and different word order patterns will favor different regularities. For instance, prepositions favor backward predictability (in “in London”, “in” is more likely to be followed by any city (i.e., IN-LOCATION), while “London” has few preceding candidates); instead, post-positions are more likely to yield predictable forward patterns (LOCATION-IN).

In this work, we investigated how typological variability influences the acquisition of adjectives in their interplay with nouns. We focus on two languages: English, in which adjectives occur pre-nominally, and Italian, where adjectives can occur either pre-nominally or post-nominally, albeit with preferential restrictions. We present two studies: Study 1 is an analysis of the predictability of adjectives in child directed input in each language; and Study 2 presents computational modeling work investigating whether these dependencies are captured by a learning model during online processing.

Study 1 used naturalistic data from the English and Italian corpora in CHILDES (McWhinney, 2000). Using the lemmatized version of the words, we analyzed the statistical dependencies between descriptive adjectives and nouns. This required annotation of syntactic categories, which was conducted using an automatic part-of-speech tagger, followed by additional manual revision. We then computed the transitional probabilities (TPs) between adjectives and nouns, and between nouns and adjectives (the latter only in the case of Italian). Figure 1 shows that, whereas forward TPs better predict the adjectives occurring in the Italian canonical N-Adj ordering, the opposite is the case for English. In the case of the Adj-N order in Italian, both forward and backward probabilities are equally informative, possibly due to the highly formulaic nature of this syntactic pattern. 

Typological effects on statistical dependencies have been previously identified (Onnis&Thiessen, 2013), but discussion of them in models of distributional language learning is sparse. To this end, Study 2 aimed to establish whether a Recurrent Neural Network (RNN), a well established account of language learning and processing, is sensitive to these cross-linguistic differences. An analysis of the Age of Acquisition (AoA) norms in Wordbank (Frank et al. 2017) revealed no statistically significant difference in the acquisition of adjectives in Italian and English; therefore, if the RNN can capture both types of dependencies, it should learn adjectives in each language equally well. 

We trained the RNN on the the child-directed speech data from Study 1. Since this model processes language incrementally in the forward direction, we expected this model to favor forward over backward dependencies. To test this, we also implemented a variant of the RNN that experiences language in the reverse direction (we refer to it as bwRNN). Although this model is not cognitively plausible, its relative success in the prediction of adjectives would provide an indication for the necessity of extending the standard RNN model to account for retrodiction during language learning.  As can be seen in Figure 2, at the end of training, the standard RNN is less successful in learning English than Italian (where success is quantified by low entropy), suggesting that the standard RNN is not capturing the backward dependencies in English. In contrast, the bwRNN performed much better for English –consistent with the reliability of backward over forward dependencies in this language– and had greater variability for Italian. These results suggest a full account of language acquisition will have to account for typological influences on distributional information, which likely require children to attend to statistical dependencies that both predict and retrodict.
